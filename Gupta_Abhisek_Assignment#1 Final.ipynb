{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "canopy_exercise": {
     "cell_type": "<None>"
    }
   },
   "source": [
    "## Assignment 1: Data Harvesting from Yelp.com (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "canopy_exercise": {
     "cell_type": "question"
    }
   },
   "source": [
    "### Your task is to collect the name, number of reviews, overall rating and price range ('$$' signs) for West Lafayette restaurants from yelp.com. You may consider manipulating this link for accomplishing the task: \"http://www.yelp.com/search?find_desc=Restaurants&find_loc=West+Lafayette,+IN&start=0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "canopy_exercise": {
     "cell_type": "question"
    }
   },
   "source": [
    "Include the name of anyone you worked with (you are allowed to partner with another peer):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here are the libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare the lists that will hold all the data we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name = []\n",
    "number_reviews = []\n",
    "rating = []\n",
    "price_range = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "webaddress = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "canopy_exercise": {
     "cell_type": "hint"
    }
   },
   "source": [
    "#### Create code that will go through the list of restaurants and finally write the data in a file named \"Yelp_Assignment1_Output.csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "canopy_exercise": {
     "cell_type": "hint"
    }
   },
   "source": [
    "Helpful tips:\n",
    "\n",
    "Make sure you sleep as you paginate through the pages.\n",
    "\n",
    "Do NOT use BeautifulSoup. It does not work!!!\n",
    "\n",
    "Use .find() to locate a suitable delimiter and exploit that to extract the desired values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib import FancyURLopener\n",
    "from random import choice\n",
    "\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "    'Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16',\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 6.1; rv:2.2) Gecko/20110201',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246'\n",
    "]\n",
    "\n",
    "#The user agents are just the version strings of different browsers. For our example, we have obtained the string from \"http://useragentstring.com/index.php?id=19841\"\n",
    "\n",
    "class MyOpener(FancyURLopener, object):\n",
    "    version = choice(user_agents)\n",
    "\n",
    "myopener = MyOpener()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in range(0,37):   \n",
    "    time.sleep(2)\n",
    "    url = 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=West+Lafayette,+IN&start='+str(x*10)\n",
    "            \n",
    "    #Find the URL's of all the listings on this page\n",
    "\n",
    "    myopener = MyOpener()\n",
    "    page=myopener.open(url)\n",
    "    Source_Code = page.read()\n",
    "    for x in range(0,10):\n",
    "        #time.sleep(10)\n",
    "        index = Source_Code.find(\"biz-name\\\" href=\\\"/biz/\") #Source_Code is assumed to have the html text\n",
    "        Source_Code1 = Source_Code[index:]\n",
    "        index = Source_Code1.find(\"href\")\n",
    "        Source_Code1 = Source_Code1[index+6:]\n",
    "        Source_Code = Source_Code1\n",
    "        index = Source_Code1.find(\"data-hovercard-id\")\n",
    "        Source_Code1 = Source_Code1[1:index-2]\n",
    "        Source_Code1 = Source_Code1.replace(\"\\n\",\"\").strip()\n",
    "        webaddress.append(Source_Code1)\n",
    "webaddress1 = webaddress\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(webaddress))\n",
    "webaddress = filter(None, webaddress)\n",
    "len(webaddress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls_main = []\n",
    "for an_address in webaddress:\n",
    "    urls_main.append('https://www.yelp.com/' + str(an_address))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "canopy_exercise": {
     "cell_type": "starting code"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370\n",
      "370\n",
      "370\n",
      "370\n"
     ]
    }
   ],
   "source": [
    "for x in urls_main:\n",
    "    page=myopener.open(x)\n",
    "    Source_Code = page.read()\n",
    "     \n",
    "    # name :\n",
    "    index = Source_Code.find(\"biz-page-title embossed-text-white\") #Source_Code is assumed to have the html text\n",
    "    Source_Code1 = Source_Code[index:]\n",
    "    index = Source_Code1.find(\">\")\n",
    "    Source_Code1 = Source_Code1[index:]\n",
    "    index = Source_Code1.find(\"<\")\n",
    "    Source_Code1 = Source_Code1[1:index]\n",
    "    Source_Code1 = Source_Code1.replace(\"\\n\",\"\").strip()\n",
    "    name.append(Source_Code1)\n",
    "    \n",
    "    #number of reviews:\n",
    "    index = Source_Code.find(\"review-count rating-qualifier\") #Source_Code is assumed to have the html text\n",
    "    Source_Code1 = Source_Code[index:]\n",
    "    index = Source_Code1.find(\">\")\n",
    "    Source_Code1 = Source_Code1[index:]\n",
    "    index = Source_Code1.find(\"<\")\n",
    "    Source_Code1 = Source_Code1[1:index]\n",
    "    Source_Code1 = Source_Code1.replace(\"\\n\",\"\").strip()\n",
    "    number_reviews.append(Source_Code1)\n",
    "    \n",
    "    # Price range \n",
    "    index = Source_Code.find(\"business-attribute price-range\") #Source_Code is assumed to have the html text\n",
    "    Source_Code1 = Source_Code[index:]\n",
    "    index = Source_Code1.find(\">\")\n",
    "    Source_Code1 = Source_Code1[index:]\n",
    "    index = Source_Code1.find(\"<\")\n",
    "    Source_Code1 = Source_Code1[1:index]\n",
    "    Source_Code1 = Source_Code1.replace(\"\\n\",\"\").strip()\n",
    "    price_range.append(Source_Code1)\n",
    "    \n",
    "    #rating\n",
    "    url1 = \"https://www.yelp.com/biz/maru-japanese-sushi-restaurant-west-lafayette\"\n",
    "    page=myopener.open(url1)\n",
    "    Source_Code = page.read()\n",
    "    index = Source_Code.find(\"star-img\") #Source_Code is assumed to have the html text\n",
    "    Source_Code1 = Source_Code[index:]\n",
    "    index = Source_Code1.find(\"title=\")\n",
    "    Source_Code1 = Source_Code1[index:]\n",
    "    index = Source_Code1.find(\">\")\n",
    "    Source_Code1 = Source_Code1[7:index-1]\n",
    "    Source_Code1 = Source_Code1.replace(\"\\n\",\"\").strip()\n",
    "    rating.append(Source_Code1)\n",
    "print len(number_reviews)\n",
    "print len(name)\n",
    "print len(price_range)\n",
    "print len(rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't forget to write all the collected data to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_file = open(\"C:/Users/abhis/Downloads/Yelp_Assignment_Abhisek.csv\",\"w\")\n",
    "\n",
    "out_file.write(\"Name , Number Of Reviews , Price Range, Rating, \\n\")\n",
    "\n",
    "for x in range(0 , len(urls_main)):\n",
    "    \n",
    "    out_file.write(name[x] + \" , \" + number_reviews[x] + \" , \" + price_range[x] + \" , \" + rating[x] +  \"\\n\" )\n",
    "     \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Data Harvesting from Zillow.com (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your task is to collect the current property listings for West Lafayette from zillow.com. In particular, collect the listings from the first three pages and then collect details about everyhouse. We want the number of bedrooms, bathrooms, sqaure footage, zestimate, and rent zestimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here are the libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare the lists that will hold all the data we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "webaddresses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addresses = []\n",
    "beds = []\n",
    "baths = []\n",
    "sqrft = []\n",
    "zest = []\n",
    "zrent = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create code that will go through the first 3 pages of Zillow.com listings for West Lafayette.  For each page, obtain all the listings on that page.  Then, store those urls in a list called urls.  Now, go through each url in the list (which will take you to the page of the specific address in question) and allow you to obtain the information that we need.  If the data is not available, then type in \"Not Available.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for page in range(1 , 4): # set the correct page range to traverse through first three pages of listings.\n",
    "\n",
    "    #this link gives you the new listings in relevance sort order.  You really don't need to change anything here.\n",
    "    source_code = urllib2.urlopen(\"http://www.zillow.com/homes/for_sale/West-Lafayette-IN/41643_rid/globalrelevanceex_sort/40.926483,-86.488495,40.365904,-87.426453_rect/9_zm/\"+str(page)+\"_p/\").read()\n",
    "    \n",
    "    soup_main_page = BeautifulSoup(source_code, 'lxml')\n",
    "    \n",
    "    #Get all the URLs and store in the list below\n",
    "    \n",
    "    urls = []\n",
    "    \n",
    "    #find the result set that has the links to each individual listing\n",
    "    # think about using .find_all with the right tag and class attribute\n",
    "    for x in soup_main_page.find_all(class_='zsg-photo-card-overlay-link routable hdp-link routable mask hdp-link',href=True):\n",
    "        y = x['href']\n",
    "        urls.append(y)\n",
    "    # once this is done, you may use a for loop to populate the urls list\n",
    "    # done above\n",
    "    \n",
    "    for url in urls:\n",
    "    \n",
    "        url = 'http://www.zillow.com'+url #this creates the correct the url for going to a house page, provided urls list has the \n",
    "        #right endings\n",
    "        \n",
    "        source_code = urllib2.urlopen(url).read()\n",
    "        \n",
    "        soup_address_page = BeautifulSoup(source_code,'lxml')\n",
    "        \n",
    "    #given correct url endings in the urls list, you may start a new for loop to go to every house page and collect\n",
    "    #the information you need\n",
    "        \n",
    "        #find the address\n",
    "        for x in soup_address_page.find_all('h1', class_= 'notranslate'):\n",
    "            a = x.text.replace(\",\",\"\")\n",
    "            addresses.append(a)  \n",
    "        \n",
    "        #find the beds baths and squarefeet\n",
    "        for y in soup_address_page.find_all('span', class_='addr_bbs'):      \n",
    "            z = y.text\n",
    "            x = str(z.split()[0])\n",
    "            if z.split()[1] == 'beds':\n",
    "                beds.append(x)\n",
    "            elif z.split()[1] == 'baths':\n",
    "                baths.append(x)\n",
    "            elif z.split()[1] == 'sqft':\n",
    "                y = x.replace(\",\",\"\")\n",
    "                sqrft.append(y)\n",
    "        m=0\n",
    "        #find the zrent and zestimate:\n",
    "        for zestimate_temp in soup_address_page.find_all(\"div\" , class_ = \"zest-value\"):   \n",
    "            if m == 0:\n",
    "                zest.append(str(zestimate_temp.text).replace(\",\",\"\"))\n",
    "            elif m == 1:\n",
    "                zrent.append(str(zestimate_temp.text).replace(\",\",\"\"))\n",
    "            m=m+1\n",
    "        \n",
    "        \n",
    "        #this sleep important not to overload zillow site. we are sleeping after every request to a house page\n",
    "       \n",
    "        time.sleep(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we write all of our output file to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file = open('C:/Users/abhis/Downloads/Zillow_Assignment1_Output.csv' , 'w')\n",
    "\n",
    "output_file.write(\"Address,Beds,Baths,Squarefeet,Zestimate,Zrent\\n\")\n",
    "\n",
    "for i in range(0 , len(beds)-2):\n",
    "    \n",
    "    output_file.write( addresses[i] + \",\" + beds[i] +\",\" + baths[i] + \",\" + sqrft[i] + \",\" + zest[i] + \",\" + zrent[i] + \"\\n\" )\n",
    "    \n",
    "output_file.close()\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
